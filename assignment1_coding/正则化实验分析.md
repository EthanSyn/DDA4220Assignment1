# 正则化系数对比实验结果

## 实验结果

| 正则化系数 | 优化器 | Training Acc | Validation Acc | Test Acc |
|-----------|--------|--------------|----------------|----------|
| 1.0       | SGD    | 0.1044       | 0.1060         | 0.1028   |
| 1.0       | AdamW  | 0.8277       | 0.8238         | 0.8633   |
| 0.1       | SGD    | 0.3371       | 0.3297         | 0.3756   |
| 0.1       | AdamW  | 0.8482       | 0.8536         | 0.8735   |
| 0.01      | SGD    | 0.8841       | 0.8920         | 0.8920   |
| 0.01      | AdamW  | 0.8659       | 0.8438         | 0.8858   |
| 0.001     | SGD    | 0.8841       | 0.8920         | 0.8920   |
| 0.001     | AdamW  | 0.8659       | 0.8438         | 0.8858   |
| 0.0001    | SGD    | 0.9294       | 0.9338         | 0.9334   |
| 0.0001    | AdamW  | 0.7624       | 0.7672         | 0.7595   |

## 训练现象分析

### SGD优化器

**α=1.0（强正则化）**: 正则化过强导致严重的欠拟合，模型几乎无法学习，准确率仅为10.28%，接近随机猜测水平。过大的正则化惩罚项完全抑制了权重的增长，使模型失去表达能力。

**α=0.1（中强正则化）**: 仍然存在明显的欠拟合，测试准确率仅为37.56%。正则化力度依然过大，严重限制了模型的学习能力，训练和验证准确率都很低且接近。

**α=0.01（适度正则化）**: 模型达到良好平衡，测试准确率为89.20%。训练、验证和测试准确率非常接近，表明模型泛化性能良好，既没有过拟合也没有欠拟合。

**α=0.001（弱正则化）**: 与α=0.01表现完全相同，说明在这个数据规模和模型复杂度下，0.001和0.01的正则化效果相当，都能提供足够的正则化约束。

**α=0.0001（很弱正则化）**: 达到最佳性能，测试准确率为93.34%。正则化较弱允许模型充分学习数据特征，且由于数据集规模较大、模型相对简单，即使正则化很弱也未出现明显过拟合。验证准确率（93.38%）略高于训练准确率（92.94%），表明泛化能力优秀。

### AdamW优化器

**α=1.0（强正则化）**: 与SGD不同，AdamW表现出更好的鲁棒性，测试准确率达到86.33%。AdamW的自适应学习率和动量机制部分补偿了过强正则化带来的负面影响，但训练准确率（82.77%）与验证准确率（82.38%）较为接近，依然存在一定的欠拟合。

**α=0.1（中强正则化）**: 性能进一步提升至87.35%，AdamW展现出比SGD更强的适应能力。自适应学习率帮助模型在较强正则化约束下仍能有效学习。

**α=0.01（适度正则化）**: 测试准确率为88.58%，略低于α=0.1。此时训练准确率（86.59%）高于验证准确率（84.38%），出现轻微过拟合迹象，说明对于AdamW这个正则化强度可能略显不足。

**α=0.001（弱正则化）**: 与α=0.01表现完全相同，固定随机种子导致相同的训练轨迹。过拟合现象同样存在。

**α=0.0001（很弱正则化）**: 出现明显过拟合，测试准确率下降至75.95%。训练准确率（76.24%）虽然不是最高，但验证准确率（76.72%）显著低于较强正则化时的表现。AdamW在弱正则化下容易过拟合，这与其自适应学习率机制有关——优化器能够快速适应训练数据的细节特征，但缺乏足够的正则化约束时会导致泛化能力下降。

### 关键观察

1. **优化器差异**: SGD在弱正则化下表现最佳（α=0.0001，93.34%），而AdamW在中等正则化下表现最佳（α=0.1，87.35%）。两种优化器对正则化强度的需求明显不同。

2. **正则化敏感性**: SGD对强正则化极其敏感，α≥0.1时出现严重欠拟合。AdamW则表现出更好的鲁棒性，在α=1.0时仍能达到86%以上的准确率。

3. **过拟合风险**: AdamW在弱正则化（α≤0.001）下容易过拟合，而SGD直到α=0.0001仍保持良好泛化。这反映了自适应优化器的双刃剑特性——快速收敛的同时也更容易记忆训练数据。

4. **最优配置**:
   - SGD: α=0.0001时最佳（Test Acc: 93.34%）
   - AdamW: α=0.1时最佳（Test Acc: 87.35%）
   - 总体最佳: SGD + α=0.0001

5. **泛化性能**: SGD在弱正则化下的泛化能力优于AdamW，验证和测试准确率都更接近训练准确率，说明SGD的隐式正则化效果更强。

